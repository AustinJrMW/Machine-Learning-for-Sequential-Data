{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Types of Sequential Data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In common machine learning tasks, it is assumed that the data is identically and independently distributed (i.i.d.). However, when dealing with changes in distribution of the underlying data generating process, or working with data that has temporal dependence, this i.i.d. assumption breaks.\n",
    "\n",
    "Practitioners and data scientists should be able to model such data by drawing from a variety of tools for sequential data analysis. In this lab, we introduce forms of sequential data, and basic concepts needed to understand components of a time-series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"https://#Objectives\">Objectives</a></li>\n",
    "    <li><a href=\"https://#Datasets\">Datasets</a></li>\n",
    "    <li>\n",
    "        <a href=\"https://#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"https://#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
    "            <li><a href=\"https://#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "            <li><a href=\"https://#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"https://#Background\">Background</a>\n",
    "        <ol></ol>\n",
    "    </li>\n",
    "    <li><a href=\"https://#The i.i.d. Assumption\">The i.i.d. Assumption</a>\n",
    "        <ol></ol>\n",
    "    </li>\n",
    "    <li><a href=\"https://#Understanding Sequential Modeling\">Understanding Sequential Modeling</a>\n",
    "        <ol>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"https://#Forms of Sequential Data\">Forms of Sequential Data</a>\n",
    "        <ol>\n",
    "            <li><a href=\"https://#Working with time-series data\">Working with time-series data</a></li>\n",
    "            <li><a href=\"https://#Working with text data\">Working with text data</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"https://#Example 1 - Working with Speech Commands\">Example 1 - Working with Speech Commands</a>\n",
    "        <ol>\n",
    "            <li><a href=\"https://#Exercise A - Import data set\">Exercise A - Import data set</a></li>\n",
    "            <li><a href=\"https://#Exercise B - Extract audio clips\">Exercise B - Extract audio clips</a></li>\n",
    "            <li><a href=\"https://#Exercise C - Plotting audio clips\">Exercise C - Plotting audio clips</a></li>\n",
    "            <li><a href=\"https://#Exercise D - Basic pre-processing\">Exercise D - Basic pre-processing</a></li>\n",
    "            <li><a href=\"https://#Exercise E - Spectrogram\">Exercise E - Spectrogram</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"https://#Example 2 - Gene Family Classification\">Example 2 - Gene Family Classification</a>\n",
    "        <ol>\n",
    "            <li><a href=\"https://#Exercise A - Load human DNA data\">Exercise A - Load human DNA data</a></li>\n",
    "            <li><a href=\"https://#Exercise B - Sequences of Characters into K-mers\">Exercise B - Sequences of Characters into K-mers</a></li>\n",
    "            <li><a href=\"https://#Exercise C - Bag-of-Words\">Exercise C - Bag-of-Words</a></li>\n",
    "            <li><a href=\"https://#Exercise D - Multinomial Naïve Bayes Classifier\">Exercise D - Multinomial Naïve Bayes Classifier</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, the following libraries were used:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n",
    "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n",
    "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the following code cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are **not** pre-installed in the Skills Network Labs environment. **You will need to run the following cell** to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install nltk "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "print(tf. __version__)\n",
    "import skillsnetwork\n",
    "\n",
    "import keras \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from dateutil.parser import parse\n",
    "import os\n",
    "import pathlib\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "In machine learning, it is quite common to assume that the data are identically and independently distributed or i.i.d. This implies that the generative process does not have any memory of past samples to generate new samples.\n",
    "This assumption is usually violated when dealing with sequential data as samples depend on past information, that is, they are reliant on one another due to their sequential order.\n",
    "Examples where data depends on past information may include weather. For instance, it's not likely to snow if the temperature is extremely hot the day before. In financial time series, the price of the stock is not only determined by the fundamentals of the company but the price of the stock the day before; we will see more examples later on.\n",
    "There are many method to study time series data; in this section, we will study some of the classic methods. These methods are precursors to Recurrent Neural Networks (RNNs), a well-known method, well-suited to model these forms of sequential data that we will cover in the next few sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The i.i.d. Assumption\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project, is dealing with data that, rather than being drawn i.i.d., from some joint distribution $P(x, y)$, actually consists of sequences of $(x, y)$ pairs that show some sequential correlation. This means that values close to the x and y values are likely to be related to and/or dependent on each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when the points in the data set are dependent on the other points in the data set, the data is termed sequential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential supervised learning problem can be formulated as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let ${(x_i, y_i)}^N\\_{i=1}$ be a set of N training examples. In a part-of-speech tagging task, one $(x_i, y_i)$ pair might consist of $x_i =$ do you want fries and $y_i =$ verb pronoun verb noun. Our goal is to build a model, $h$, that predicts the next label sequence, $y = h(x)$, given an input sequence, $x$. Here the task is to predict the $t+1$st element of the sequence $(y\\_1,...,y_t)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us walk through an example to highlight the issues when working with non-i.i.d. data. We will be using financial quotations from various energy companies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/datasets/financial-data.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "symbols = {\"TOT\": \"Total\", \"XOM\": \"Exxon\", \"CVX\": \"Chevron\",\n",
    "           \"COP\": \"ConocoPhillips\", \"VLO\": \"Valero Energy\"}\n",
    "template_name = (\"./financial-data/{}.csv\")\n",
    "\n",
    "quotes = {}\n",
    "for symbol in symbols:\n",
    "    data = pd.read_csv(\n",
    "        template_name.format(symbol), index_col=0, parse_dates=True\n",
    "    )\n",
    "    quotes[symbols[symbol]] = data[\"open\"]\n",
    "quotes = pd.DataFrame(quotes)\n",
    "quotes .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot a few financial quotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "quotes.plot()\n",
    "plt.ylabel(\"Quote value\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")\n",
    "_ = plt.title(\"Stock values over time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict the quotation of Chevron using all other energy companies’ quotes. We will use the decision tree regressor that is expected to overfit and thus not generalize to unseen data. We will start off by splitting our data into a testing and training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = quotes.drop(columns=[\"Chevron\"]), quotes[\"Chevron\"]\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have our features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and target Chevron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now define our model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we apply a cross-validation strategy, to check the generalization performance of our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(data_train, target_train)\n",
    "target_predicted = regressor.predict(data_test)\n",
    "# Affect the index of `target_predicted` to ease the plotting\n",
    "target_predicted = pd.Series(target_predicted, index=target_test.index)\n",
    "target_predicted "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "test_score = r2_score(target_test, target_predicted)\n",
    "print(f\"The R2 on this single split is: {test_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get outstanding generalization performance in terms of $R^2$. Let's plot the predictions against the ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train.plot(label=\"Training\")\n",
    "target_test.plot(label=\"Testing\")\n",
    "target_predicted.plot(label=\"Prediction\")\n",
    "\n",
    "plt.ylabel(\"Quote value\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")\n",
    "_ = plt.title(\"Model predictions using a ShuffleSplit strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are surprisingly very good, as we had originally expected the model to overfit and not generalize on unseen test sets. Let us now investigate why this is the case.\n",
    "\n",
    "We used a cross-validation method that shuffles data and splits. We will simplify this procedure by not shuffling our data and plotting the results obtained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data, target, shuffle=False, random_state=0,\n",
    ")\n",
    "regressor.fit(data_train, target_train)\n",
    "target_predicted = regressor.predict(data_test)\n",
    "target_predicted = pd.Series(target_predicted, index=target_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = r2_score(target_test, target_predicted)\n",
    "print(f\"The R2 on this single split is: {test_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train.plot(label=\"Training\")\n",
    "target_test.plot(label=\"Testing\")\n",
    "target_predicted.plot(label=\"Prediction\")\n",
    "\n",
    "plt.ylabel(\"Quote value\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")\n",
    "_ = plt.title(\"Model predictions using a split without shuffling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that our model performs worse than just predicting the mean of the target. Why is that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, and all other similar time series datasets, the testing samples follow some of the training sample. As mentioned before, in time-series subsequent samples are dependent on previous samples; that is, there is a relationship between sample $t$ and sample $t+1$. We are clearly violating the i.i.d. assumption. The model's prediction would be close to true values even when it doesn't learn anything, because of data leakage and because it could have just memorized the training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Sequential Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential modeling can be described as the process of forecasting a sequence of values from a set of input values, similar to what we saw in the previous section.\n",
    "\n",
    "For example, consider a time-series that represents how a patient's physiological values like continuous heart rate, ECG, breathing rate, and temperature, change over time. One possible sequential modeling task could be to forecast their disease trajectory or predict their medical condition at some future time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text prediction is another example. For example, you can be given a sequence of words, and your modeling task could be to predict the next word based on the sequence of the previous word/phrase and a set of pre-loaded conditions and rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forms of Sequential Data\n",
    "\n",
    "Sequential data contains elements that are ordered into sequences. For example, time series (like stock values or sensor measurements), gene sequences (${C,G,A,T}$), speech, text (${a,...,z,0,...,9,...}$), video clips, and musical notes, and so on.\n",
    "\n",
    "To summarize, sequential data has some temporal coherence, and can be of arbitrary lengths. A lot of tasks can be modeled from these types of data. For example:\n",
    "\n",
    "*   text classification, such as spam email or not\n",
    "*   language translation, such as French to English\n",
    "*   time-series forecasting, such as stock prices prediction\n",
    "\n",
    "Let us look at a few common sequential data sets, and understand pre-processing techniques associated with each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with time-series data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series are special types of sequences that consist of random variables indexed by time. In particular, the random variables can be dependent and their distribution might change over time, so time-series also violate the i.i.d. assumption.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time-series decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series data can exhibit various patterns, and it is often helpful to split a time series into several components, each representing an underlying pattern category. These include trend, seasonality, and cycles, which we will now explain using examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **trend**: A trend is observed when there is an increasing or decreasing slope observed in the time series. In the following time series, we see an increasing pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/guinearice.csv', parse_dates=['date'], index_col='date')\n",
    "plt.plot(trend.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **seasonality**: Seasonality represents a distinct repeated pattern observed between regular intervals due to seasonal factors, like month of the year, the day of the month, weekdays or even time of the day, festivals, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/sunspotarea.csv', parse_dates=['date'], index_col='date')\n",
    "plt.plot(seasonality.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A time series can have both trend and seasonality, for example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality_trend = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/AirPassengers.csv', parse_dates=['date'], index_col='date')\n",
    "plt.plot(seasonality_trend.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **cyclic**: Cyclicity is similar to seasonality, but it happens when the rise and fall pattern in the series does not happen in fixed calendar-based intervals, that is, if the patterns are not of fixed calendar-based frequencies, then the pattern is cyclic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A time series can be modeled as an additive or multiplicative, wherein each observation in the series can be expressed as either a sum or a product of the components.\n",
    "\n",
    "**Additive time series** \\\n",
    "\\\n",
    "$Value(t) = BaseLevel(t) + Trend(t) + Seasonality(t) + Error(t)$\n",
    "\n",
    "**Multiplicative Time Series** \\\n",
    "\\\n",
    "$Value(t) = BaseLevel (t)x Trend(t) x Seasonality(t) x Error(t)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use functions like `seasonal_decompose` from the python package, `statsmodel`, to decompose our time-series into trend, seasonality, and the residual components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/a10.csv', parse_dates=['date'], index_col='date')\n",
    "plt.plot(df.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by performing additive decomposition. Note that by using `extrapolate_trend = 'freq'`, we impute missing values at the beginning of the time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive Decomposition\n",
    "result_add = seasonal_decompose(df['value'], model='additive', extrapolate_trend='freq')\n",
    "\n",
    "# Plot\n",
    "result_add.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similarly we can try performing multiplicative decomposition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicative Decomposition \n",
    "result_mul = seasonal_decompose(df['value'], model='multiplicative', extrapolate_trend='freq')\n",
    "\n",
    "#Plot\n",
    "result_mul.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, we see that with additive decomposition, some pattern is still left over, while with multiplicative decomposition, the result looks quite random, which is what we want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we detrend our time series? One way would be to subtract the trend component obtained from the time series decomposition we saw earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended = df.value.values - result_mul.trend\n",
    "plt.plot(detrended)\n",
    "plt.title('Drug Sales detrended by subtracting the trend component', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can remove the seasonality component:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deseasonalized = df.value.values / result_mul.seasonal\n",
    "\n",
    "# Plot\n",
    "plt.plot(deseasonalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time-series imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we briefly mentioned imputing missing values in a time-series. Sometimes, your time series will have missing dates/times. That means the data was not captured or unavailable for those periods.\n",
    "\n",
    "Depending on the nature of the series, we can try multiple approaches for imputation:\n",
    "\n",
    "*   Forward\n",
    "*   Backward Fill\n",
    "*   Linear Interpolation, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by parsing the data frame indices as dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/a10.csv', parse_dates=['date'], index_col='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **Forward Imputation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df_ffill = df.ffill()\n",
    "# Print the MSE between imputed value and ground truth\n",
    "error = np.round(mean_squared_error(df['value'], df_ffill['value']), 2)\n",
    "df_ffill['value'].plot(title='Forward Fill (MSE: ' + str(error) +\")\", label='Forward Fill', style=\".-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **Backward Imputation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bfill = df.bfill()\n",
    "error = np.round(mean_squared_error(df['value'], df_bfill['value']), 2)\n",
    "df_bfill['value'].plot(title=\"Backward Fill (MSE: \" + str(error) +\")\", label='Back Fill', color='firebrick', style=\".-\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **Linear Interpolation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rownum'] = np.arange(df.shape[0])\n",
    "df_nona = df.dropna(subset = ['value'])\n",
    "f = interp1d(df_nona['rownum'], df_nona['value'])\n",
    "df['linear_fill'] = f(df['rownum'])\n",
    "error = np.round(mean_squared_error(df['value'], df['linear_fill']), 2)\n",
    "df['linear_fill'].plot(title=\"Linear Fill (MSE: \" + str(error) +\")\",label='Cubic Fill', color='green', style=\".-\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time-series decomposition and imputation are common pre-processing steps used when working on time-series prediction or forecasting tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with text data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be working with a portion of the [Twitter Sentiment Analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01) dataset from Kaggle.\n",
    "\n",
    "> Given a message and an entity, the task is to judge the sentiment of the message about the entity. There are three classes in this dataset: Positive, Negative, and Neutral.\n",
    "\n",
    "The file contains the tweet along with the labels. For the purpose of this lab, we will just explore the dataset, and perform some basic pre-processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us download this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Id', 'Entity', 'Sentiment', 'Tweet']\n",
    "df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/twitter_validation.csv', names = cols, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `groupby` in pandas, we can determine the number of tweets present in each category and plot them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "df.groupby(['Sentiment']).Tweet.count().sort_values().plot.barh(\n",
    "    ylim=0, title= 'tweets per category')\n",
    "\n",
    "plt.xlabel('# of occurrences', fontsize = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will focus on preprocessing the raw tweets. This step is important because raw tweets without preprocessing are highly unstructured and contain redundant and often problematic information. They contain tons of noise that should be removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the text data is cleaned by following the steps below.\n",
    "\n",
    "*   Remove punctuations\n",
    "*   Remove stopwords\n",
    "*   Tokenization - Converting a sentence into list of words\n",
    "*   Lemmatization and stemming - Tranforming any form of a word to its root word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a corpus of stop-words, including articles, prepositions, and conjunctions, from the Natural Language Toolkit (NLTK) suite, words with little lexical content (such as the, a, also, from, and so on) can be filtered out, for more check out this <a href=\"https://realpython.com/nltk-nlp-python/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01\">link</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    regular_punct = list(string.punctuation)\n",
    "    for punc in regular_punct:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, ' ')\n",
    "    return text.strip().lower()\n",
    "\n",
    "df['Tweet'] = df['Tweet'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tweet):\n",
    "    en_stops = set(stopwords.words('english'))\n",
    "    tweet = tweet.split()\n",
    "    tweet = \" \".join([word for word in tweet if not word in en_stops])  \n",
    "    return tweet\n",
    "\n",
    "df['Tweet'] = df['Tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets can be further processed with tokenization and lemmatization, both of which were performed using packages in the NLTK suite.\n",
    "\n",
    "Tokenization converts text to analysis relevant word tokens, while lemmatization transforms words to a simpler form, returning the word’s lemma – a canonical form of all its inflectional forms (for example, go represents its inflected forms of goes, going, went, gone).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet'] = df['Tweet'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_wordnet(input):\n",
    "    lem = WordNetLemmatizer()\n",
    "    return [lem.lemmatize(w) for w in input]\n",
    "\n",
    "df['Tweet'] = df['Tweet'].apply(lemma_wordnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explore two techniques for encoding text data into numerical vectors. This is to extract suitable features from processed texts. Both will be implemented using scikitlearn’s CountVectorizer, where `ngram_range` will be set to (1,1) for BoW and (2,2) for B-BoW.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-Words (BoW) is a vectorization method to convert variable-length texts into fixed-length vectors, without considering the semantic relation between words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW often results in a very sparse representation. For a large dataset with the vocabulary of a few thousand words, preprocessing text before employing BoW can be useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine individual words\n",
    "\n",
    "def combine_text(input):\n",
    "    combined = ' '.join(input)\n",
    "    return combined\n",
    "\n",
    "df['Tweet'] = df['Tweet'].apply(combine_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot some of the values we see they are zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 1))\n",
    "X_train_bow = cv.fit_transform(df['Tweet'])\n",
    "print(X_train_bow[0:10,0:10].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 1))\n",
    "X_train_bow = cv.fit_transform(df['Tweet'].values.tolist()) \n",
    "Y_train_bow = df['Sentiment']\n",
    "Y_train_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram BoW (B-BoW) represents a text document as a weakly ordered collection of contiguous sequences but of two items. It allows for the preservation of more word locality information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_bbow = CountVectorizer(ngram_range=(2, 2))\n",
    "X_train_bbow = cv_bbow.fit_transform(df['Tweet']) \n",
    "Y_train_bbow = df['Sentiment']\n",
    "Y_train_bbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency-inverse document frequency (TF-IDF) is a frequency-based embedding method that\n",
    "measures the importance of a word in a given document. It can be implemented using the `TfidfVectorizer` function from sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf = True, ngram_range=(1, 1))\n",
    "vectorizer.fit(df)\n",
    "X_train_tfidf = vectorizer.fit_transform(df['Tweet'])\n",
    "Y_train_tfidf = df['Sentiment'] \n",
    "Y_train_tfidf "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
